# evolution/judgment_evolver.py

def evolve_judgment(judgment: dict, outcome: dict) -> dict:
    """
    ‡∏õ‡∏£‡∏±‡∏ö worldview ‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏≤‡∏Å outcome ‡πÇ‡∏•‡∏Å‡∏à‡∏£‡∏¥‡∏á
    ‡πÉ‡∏ä‡πâ‡πÇ‡∏î‡∏¢ agent_evolver
    """

    score = outcome.get("score", 0)
    global_risk = outcome.get("global_risk", 0.5)

    # ‡∏Ñ‡πà‡∏≤ default
    judgment.setdefault("worldview", "neutral")
    judgment.setdefault("confidence", 0.5)

    # üîÅ Logic ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô worldview
    if score < -0.5 or global_risk > 0.7:
        judgment["worldview"] = "defensive"
        judgment["confidence"] = max(0.1, judgment["confidence"] - 0.1)

    elif score > 0.5 and global_risk < 0.4:
        judgment["worldview"] = "aggressive"
        judgment["confidence"] = min(0.9, judgment["confidence"] + 0.1)

    else:
        judgment["worldview"] = "neutral"

    return judgment


# ‚úÖ ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö future API / main.py (Phase ‡∏ï‡πà‡∏≠‡πÑ‡∏õ)
def evolve_from_ai(world_state: dict, ai_result: dict) -> dict:
    """
    evolve judgment ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå AI ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
    (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô wrapper ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤)
    """
    outcome = {
        "score": ai_result.get("score", 0),
        "global_risk": ai_result.get("global_risk", 0.5)
    }
    return evolve_judgment(world_state, outcome)
